{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Brief interactive demo playing with glove and polyglot embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100004/100004 [00:01<00:00, 55493.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "glove = torchtext.vocab.GloVe('6B', cache='.')\n",
    "poly = torchtext.vocab.Vectors('poly.txt', cache='.')\n",
    "from sklearn.decomposition import PCA\n",
    "poly_pca = PCA(2).fit_transform(poly.vectors)\n",
    "glove_pca = PCA(2).fit_transform(glove.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Function\n",
    "\n",
    "We can use either cosine similarity (the angle between two vectors), or\n",
    "euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest(vectors, q):\n",
    "    n_vecs = vectors.vectors.shape[0]\n",
    "    distances = F.cosine_similarity(vectors.vectors, q.repeat((n_vecs, 1)))\n",
    "#     distances = -1 * F.pairwise_distance(vectors.vectors, q.repeat((n_vecs, 1)))\n",
    "    topk = distances.topk(10, largest=True)[1]\n",
    "    for i in topk:\n",
    "        print(vectors.itos[i])\n",
    "    return topk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In embedding space, nearby words tend to be similar semantically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a33ab42439b4ab7a0f4043411914980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Embeddings', options=('Polyglot', 'Glove'), value='Polyglot')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_nearest(embs, word):\n",
    "    if embs == 'Polyglot':\n",
    "        vectors = poly\n",
    "        dims = poly_pca\n",
    "    else:\n",
    "        vectors = glove\n",
    "        dims = glove_pca\n",
    "        \n",
    "    topk = nearest(vectors, vectors[word])\n",
    "    ixs = [int(i) for i in topk]\n",
    "    points = dims[ixs]\n",
    "    f, a = plt.subplots(1)\n",
    "    a.scatter(points[:, 0], points[:, 1])\n",
    "    strs = [vectors.itos[i] for i in ixs]\n",
    "    for i, txt in enumerate(strs):\n",
    "        a.annotate(txt, (points[i, 0], points[i, 1]))\n",
    "        \n",
    "embs = widgets.ToggleButtons(options=['Polyglot', 'Glove'], description='Embeddings')\n",
    "word = widgets.Text(continuous_update=False)\n",
    "interact(show_nearest, word=word, embs = embs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies\n",
    "\n",
    "Word vectors also exhibit some nice algebraic properties, where you can (seemingly) add and subtract meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bea679c15ad488b83c49c69d9ee5ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Embeddings', options=('Polyglot', 'Glove'), value='Polyglot')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analogy(embs, a, b, c):\n",
    "    if embs == 'Polyglot':\n",
    "        vectors = poly\n",
    "        dims = poly_pca\n",
    "    else:\n",
    "        vectors = glove\n",
    "        dims = glove_pca\n",
    "\n",
    "    q = vectors[b] - vectors[a] + vectors[c]\n",
    "    topk = nearest(vectors, q)\n",
    "    try:\n",
    "        ixs = [vectors.stoi[s] for s in [a, b, c]]\n",
    "        ixs = ixs + [int(x) for x in topk[:3]]\n",
    "        points = dims[ixs]\n",
    "        f, a = plt.subplots(1)\n",
    "        a.scatter(points[:, 0], points[:, 1])\n",
    "        strs = [vectors.itos[i] for i in ixs]\n",
    "        for i, txt in enumerate(strs):\n",
    "            a.annotate(txt, (points[i, 0], points[i, 1]))\n",
    "    except:\n",
    "        pass\n",
    "embs = widgets.ToggleButtons(options=['Polyglot', 'Glove'], description='Embeddings')\n",
    "a = widgets.Text(continuous_update=False, value='king')\n",
    "b = widgets.Text(continuous_update=False, value='man')\n",
    "c = widgets.Text(continuous_update=False, value='queen')\n",
    "interact(analogy, a=a, b=b, c=c, embs=embs);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
